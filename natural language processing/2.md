### Traditional Word representation algorithms
text in numberical form so that ML/DL algorithms can process it.

- one hot encoding
    - tokenise the sentences and create a vocabulary of unique words
    - represent each word as a vector of size equal to the vocabulary size with 1 at the index corresponding to the word and 0 elsewhere
    - example:
        - vocabulary: [ "I", "love", "NLP" ]
        - "I" -> [1, 0, 0]
        - "love" -> [0, 1, 0]
        - "NLP" -> [0, 0, 1]
    - drawbacks:
        - high dimensionality: as vocabulary size increases, the vector size increases, leading to sparse representations
        - no semantic meaning: does not capture relationships between words (e.g., "king" and "queen" are treated as completely unrelated)

- bag of words
    - create a vocabulary of unique words from the entire corpus
    - represent each document as a vector of word counts corresponding to the vocabulary
    - may or may not remove stop words depending on the use case (it is not removed when using TF-IDF)
    - example:
        - documents: ["I love NLP NLP", "NLP is great"]
        - vocabulary: [ "I", "love", "NLP", "is", "great" ]
        - "I love NLP NLP" -> [1, 1, 2, 0, 0]
        - "NLP is great" -> [0, 0, 1, 1, 1]
        - the above two matrix is called document term matrix
    - TDM was defined as a means of finding similar documents for the task of document classification
    - drawbacks:
        - ignores word order and context
        - high dimensionality for large vocabularies

- term frequency-inverse document frequency (TF-IDF)
    - incorporates the importance of words in a document relative to the entire corpus
    - TF: measures how frequently a term appears in a document
        - TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)
    - IDF: measures how important a term is in the entire corpus
        - IDF(t) = log_2(Total number of documents / Number of documents with term t in it)
    - TF-IDF: combines TF and IDF to give a weight to each term in a document
        - TF-IDF(t) = TF(t) * IDF(t)

#### Questions
1. Consider the following 3 documents:
    - D1: "the cat sat on the mat"
    - D2: "the dog sat on the log"
    - D3: "the cat chased the dog"
    Find the TF-IDF values for the terms the, cat, dog, sat in the document D1

    - the
        - TF(the) = 2/6
        - IDF(the) = log2(3/3) = 0
        - TF-IDF = 2/6 * 0 = 0
    - cat
        - TF(cat) = 1/6
        - IDF(cat) = log2(3/2) = 0.584
        - TF-IDF = 0.097
    - dog
        - TF(dog) = 0/6 = 0
        - IDF(dog) = log2(3/2) = 0.584
        - TF-IDF = 0 * 0.584 = 0
    - sat
        - TF(sat) = 1/6
        - IDF(sat) = log2(3/2) = 0.584
        - TF-IDF = 1/6 * 0.584 = 0.097