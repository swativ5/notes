# Introduction to Natural Language Processing

NLU: Natural Language Understanding

NLG: Natural Language Generation

What is NLP?

- Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human language in a way that is valuable.

Language Model: Given a sequence of words, predict the next word in the sequence.

Why NLP is important?

- NLP is important because it helps bridge the gap between human communication and computer understanding. It enables machines to process and analyze large amounts of natural language data, making it possible to extract meaningful insights, automate tasks, and improve user experiences in various applications such as chatbots, virtual assistants, sentiment analysis, and machine translation.

Bias: Training data may contain biases that can lead to unfair or inaccurate outcomes in NLP applications.

Applications of NLP:
- Sentiment Analysis: Determining the sentiment or emotion expressed in a piece of text, such as reviews or social media posts.
- Machine Translation: Automatically translating text from one language to another.
- Chatbots and Virtual Assistants: Enabling conversational agents to understand and respond to user queries.
- Search Engines: Improving search results by understanding user intent and context.
- Autocompletion and Spell Checking: Assisting users in writing by predicting words or correcting spelling errors
- Entity Recognition: Identifying and classifying entities (e.g., names, dates, locations) in text.
- Information Extraction in different domains: Extracting structured information from unstructured text.
- Text Summarization: Generating concise summaries of longer documents or articles.
    - Extractive Summarization: Selecting important sentences or phrases from the original text.
    - Abstractive Summarization: Generating new sentences that capture the essence of the original text.
- Spam Detection: Identifying and filtering out unwanted or malicious emails or messages.

### Important NLP Concepts

Bag-of-Words Model:- A simple representation of text data where each document is represented as a bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity.\

Stop Words:- Common words that are often filtered out in NLP tasks because they carry little meaningful information (e.g., "the", "is", "in", "and").

Term Frequency (TF):- A measure of how frequently a term appears in a document, often used in text mining and information retrieval.

Inverse Document Frequency (IDF):- A measure of how important a term is in a collection of documents, calculated as the logarithm of the total number of documents divided by the number of documents containing the term.

TF-IDF:- A statistical measure that combines Term Frequency and Inverse Document Frequency to evaluate the importance of a word in a document relative to a collection of documents.

Word Embeddings:- A type of word representation that allows words to be represented as vectors in a continuous vector space, capturing semantic relationships between words (e.g., Word2Vec, GloVe).

### Why is NLP challenging?
- Lexical Ambiguity: Words can have multiple meanings depending on the context
    - Will Will will Will's will?
    - Rose rose to put rose roes on her rows of roses.
- Syntactic Ambiguity: Sentences can be structured in multiple ways, leading to different interpretations
    - It is very warm here. (Its hot in this place vs. It is warm in personality)
    - Did you mother call your sister? I'm sure she must have. (Real people would think the call might or might not have happened but the machine would think it definetly happened)
- Structural Ambiguity: The same sentence structure can lead to different meanings
    - The man saw the boy with the binoculars. (Did the man see with the binoculars or did the boy have the binoculars?)
    - Flying planes can be dangerous. (Are the planes that are flying dangerous or is the act of flying planes dangerous?)

- I made her duck
    - I cooked a duck for her
    - I cooked duck belonging to her
    - I created the (artificial) duck, she owns
    - etc

- Code mixing: Using multiple languages in the same text or conversation, which can complicate language processing tasks.

### How do we model an NLP usign ML/DL?
Input text -> Preprocessing -> Feature Extraction -> Text Mining using ML algorithms -> Evaluation

Preprocessing:
- Tokenization: Splitting text into individual words or tokens.
- Stop Word Removal: Removing common words that do not carry significant meaning.
- Stemming: Reducing words to their root form (e.g., "running" to "run").
- Normalization: Converting text to a standard format (e.g., lowercasing, removing punctuation).
- Removing special characters, numbers, and extra spaces.
- Lemmatization: Reducing words to their base or dictionary form (e.g., "better" to "good").

Feature Extraction:
- Bag-of-Words: Representing text as a bag of its words.
- other methods

Evaluation:
- Accuracy, Precision, Recall, F1-Score: Common metrics for evaluating classification models.
- BLEU Score: A metric for evaluating the quality of machine-generated text, such as translations, by comparing it to reference texts.
- ROUGE Score: A set of metrics for evaluating automatic summarization and machine translation by comparing the overlap of n-grams between the generated text and reference texts.


Zeroshot Learning: The ability of a model to recognize and classify data points from classes it has not seen during training, based on knowledge learned from other related classes.

Fewshot Learning: A machine learning approach where a model is trained to recognize and classify new classes or tasks with only a small number of examples, leveraging prior knowledge from related tasks or classes.

Oneshot Learning: A machine learning approach where a model is trained to recognize and classify new classes or tasks with only a single example, leveraging prior knowledge from related tasks or classes.

tokenization
- the process of breaking down text into smaller units, called tokens, which can be words, phrases, or characters. This is a crucial step in NLP as it allows for easier analysis and processing of the text data.

normalization
- the process of converting text into a standard format to reduce variability and improve consistency. This can include lowercasing, removing punctuation, and converting numbers to a standard representation.
three types of normalization
- case folding: converting all text to lowercase to ensure uniformity. meaning might be lost in some cases (e.g., "Apple" the company vs "apple" the fruit).
- stemming: reducing words to their root form by only chopping off the prefix and suffix. converted word might not be a valid word (e.g., "running" to "run").
- lemmatization: reducing words to their base or dictionary form (e.g., "better" to "good").
we do stemming and lemmatization to reduce the dimensionality of the text data and improve the performance of NLP models.